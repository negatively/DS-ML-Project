# -*- coding: utf-8 -*-
"""German Traffic Signs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11oG_rJKdiuWQkcPL9tDivDONJYnFymeV

## Load Library and Dataset
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import tensorflow as tf
from keras.models import Sequential, load_model
from keras.layers import Conv2D, Dense, Flatten, Dropout, MaxPool2D
from sklearn.model_selection import train_test_split
from zipfile import ZipFile
import pickle
import seaborn as sns
import matplotlib.pyplot as plt

plt.style.use('ggplot')
# %config InlineBackend.figure_format = 'retina'

# Set Kaggle API
! pip install -q kaggle
from google.colab import files
files.upload()
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

# Get dataset from kaggle
!kaggle datasets download -d saadhaxxan/germantrafficsigns

# Extract File
with ZipFile("/content/germantrafficsigns.zip", "r") as zip_ref:
  zip_ref.extractall('working')

# Load Data
training_file = '/content/working/train.p'
testing_file = '/content/working/test.p'

# Membuka dan load data training file
with open(training_file, mode='rb') as f:
  train = pickle.load(f)

# Membuka dan load data testing file
with open(testing_file, mode='rb') as f:
  test = pickle.load(f)


print('Data Loaded')

df_sign = pd.read_csv('/content/working/signnames.csv')
SIGN_NAMES = df_sign.SignName.values
df_sign.set_index('ClassId', inplace = True)
df_sign.head(10)

# Define features and labels for training data
X, y = train['features'], train['labels']

# Converting lists into numpy arrays
data = np.array(X)
labels = np.array(y)
print(data.shape, labels.shape)

# Define features and labels for testing data
X_test, y_test = test['features'], test['labels']

# Converting lists into numpy arrays
X_test = np.array(X_test)
y_test = np.array(y_test)
print(X_test.shape, y_test.shape)

# Membagi data train menjadi train dan validation
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state = 1310)
print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)

# Visualisasi distribusi data
n_labels = np.unique(y_train).size
def hist_data(y_data, title=None, ax=None, **kwargs):
  if not ax:
    fig = plt.figure()
    ax = fig.add_subplot(111)
  ax.hist(y_data, np.arange(-0.5, n_labels+1.5  ), stacked=True, **kwargs)
  ax.set_xlim(-0.5, n_labels-1.5)
  if 'label' in kwargs : ax.legend()
  if title : ax.set_title(title)

fig, ax = plt.subplots(1, 3, figsize = (20,5))
hist_data(y_train, title = 'Distribusi kelas pada Data Training', ax = ax[0])
hist_data(y_val, title = 'Distribusi kelas pada Data Validation', ax = ax[1], color = 'black')
hist_data(y_test, title = 'Distribusi kelas pada Data Test', ax = ax[2], color = 'grey')

"""Dari plot yang tertampil, dapat dilihat bahwa distribusi kelas masing-masing bagian data terlihat mirip. Oleh karena itu, kita tidak perlu melakukan proses normalisasi. """

# Converting the labels into one hot encoding
from tensorflow.keras.utils import to_categorical

y_train = to_categorical(y_train, 43)
y_val = to_categorical(y_val, 43)

# Membuat callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') > 0.96):
      print("\nAkurasi telah mencapai > 96%. Stop Training!")
      self.model.stop_training = True
callbacks = myCallback()

# Membuat model
model = Sequential()

model.add(Conv2D(filters = 32, kernel_size = (5,5), activation = 'relu', input_shape=X_train.shape[1:]))
model.add(Conv2D(filters = 32, kernel_size = (5,5), activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(rate=0.25))

model.add(Conv2D(filters = 64, kernel_size=(3,3), activation='relu'))
model.add(Conv2D(filters = 64, kernel_size=(3,3), activation='relu'))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(43, activation='softmax'))

model.summary()

# Compile model
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# Fit the model
hist = model.fit(X_train, y_train,
                 batch_size = 32,
                 epochs = 25,
                 validation_data=(X_val, y_val),
                 callbacks=[callbacks])

# Save model
model.save('trafficsign.h5')

# Plotting graphs for accuracy 
plt.figure(0)
plt.plot(hist.history['accuracy'], label='training accuracy')
plt.plot(hist.history['val_accuracy'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()
plt.show()
 
# Plotting graphs for loss
plt.figure(1)
plt.plot(hist.history['loss'], label='training loss')
plt.plot(hist.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.show()

# Testing accuracy with test data
from sklearn.metrics import accuracy_score

y_pred = np.argmax(model.predict(X_test,), axis=-1)
accuracy_score(y_test, y_pred)

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

